[
  {
    Id: GenerateCompletion,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: true,
    Path: "/generate",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: model,
        Name: Model,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ParameterName: model,
        ArgumentName: model,
        ParameterDefaultValue: default
      },
      {
        Id: prompt,
        Name: Prompt,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The prompt to generate a response.
<br/>Example: Why is the sky blue?,
        ParameterName: prompt,
        ArgumentName: prompt,
        ParameterDefaultValue: default
      },
      {
        Id: images,
        Name: Images,
        Type: {
          CSharpType: global::System.Collections.Generic.IList<string?>?,
          IsArray: true,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        Summary: (optional) a list of Base64-encoded images to include in the message (for multimodal models such as llava),
        ParameterName: images,
        ArgumentName: images,
        ParameterDefaultValue: default
      },
      {
        Id: system,
        Name: System,
        Type: {
          CSharpType: string?,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        Summary: The system prompt to (overrides what is defined in the Modelfile).,
        ParameterName: system,
        ArgumentName: system,
        ParameterDefaultValue: default
      },
      {
        Id: template,
        Name: Template,
        Type: {
          CSharpType: string?,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        Summary: The full prompt or prompt template (overrides what is defined in the Modelfile).,
        ParameterName: template,
        ArgumentName: template,
        ParameterDefaultValue: default
      },
      {
        Id: context,
        Name: Context,
        Type: {
          CSharpType: global::System.Collections.Generic.IList<int>?,
          IsArray: true,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        Summary: The context parameter returned from a previous request to [generateCompletion], this can be used to keep a short conversational memory.,
        ParameterName: context,
        ArgumentName: context,
        ParameterDefaultValue: default
      },
      {
        Id: options,
        Name: Options,
        Type: {
          CSharpType: RequestOptions?,
          IsArray: false,
          IsEnum: false,
          Properties: [
            num_keep,
            seed,
            num_predict,
            top_k,
            top_p,
            tfs_z,
            typical_p,
            repeat_last_n,
            temperature,
            repeat_penalty,
            presence_penalty,
            frequency_penalty,
            mirostat,
            mirostat_tau,
            mirostat_eta,
            penalize_newline,
            stop,
            numa,
            num_ctx,
            num_batch,
            num_gqa,
            num_gpu,
            main_gpu,
            low_vram,
            f16_kv,
            logits_all,
            vocab_only,
            use_mmap,
            use_mlock,
            embedding_only,
            rope_frequency_base,
            rope_frequency_scale,
            num_thread
          ],
          EnumValues: null
        },
        IsRequired: false,
        Summary: Additional model parameters listed in the documentation for the Modelfile such as `temperature`.,
        ParameterName: options,
        ArgumentName: options,
        ParameterDefaultValue: default
      },
      {
        Id: format,
        Name: Format,
        Type: {
          CSharpType: string?,
          IsArray: false,
          IsEnum: true,
          Properties: [
            Json
          ],
          EnumValues: [
            json
          ]
        },
        IsRequired: false,
        Summary:
The format to return a response in. Currently the only accepted value is json.

Enable JSON mode by setting the format parameter to json. This will structure the response as valid JSON.

Note: it's important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.
,
        ParameterName: format,
        ArgumentName: format,
        ParameterDefaultValue: default
      },
      {
        Id: raw,
        Name: Raw,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        Summary:
If `true` no formatting will be applied to the prompt and no context will be returned. 

You may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API, and are managing history yourself.
,
        ParameterName: raw,
        ArgumentName: raw,
        ParameterDefaultValue: default
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        DefaultValue: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      },
      {
        Id: keep_alive,
        Name: KeepAlive,
        Type: {
          CSharpType: int,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        Summary:
How long (in minutes) to keep the model loaded in memory.

- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.
- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.
- If set to 0, the model will be unloaded immediately once finished.
- If not set, the model will stay loaded for 5 minutes by default
,
        ParameterName: keepAlive,
        ArgumentName: keepAlive,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Generate a response for a given prompt with a provided model.,
    RequestType: GenerateCompletionRequest,
    ResponseType: GenerateCompletionResponse,
    MethodName: GenerateCompletionAsync,
    NotAsyncMethodName: GenerateCompletion,
    FileNameWithoutExtension: G.Api.EndPoints.GenerateCompletion
  },
  {
    Id: GenerateChatCompletion,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: true,
    Path: "/chat",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: model,
        Name: Model,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ParameterName: model,
        ArgumentName: model,
        ParameterDefaultValue: default
      },
      {
        Id: messages,
        Name: Messages,
        Type: {
          CSharpType: global::System.Collections.Generic.IList<Message>,
          IsArray: true,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary: The messages of the chat, this can be used to keep a chat memory,
        ParameterName: messages,
        ArgumentName: messages,
        ParameterDefaultValue: default
      },
      {
        Id: format,
        Name: Format,
        Type: {
          CSharpType: string?,
          IsArray: false,
          IsEnum: true,
          Properties: [
            Json
          ],
          EnumValues: [
            json
          ]
        },
        IsRequired: false,
        Summary:
The format to return a response in. Currently the only accepted value is json.

Enable JSON mode by setting the format parameter to json. This will structure the response as valid JSON.

Note: it's important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.
,
        ParameterName: format,
        ArgumentName: format,
        ParameterDefaultValue: default
      },
      {
        Id: options,
        Name: Options,
        Type: {
          CSharpType: RequestOptions?,
          IsArray: false,
          IsEnum: false,
          Properties: [
            num_keep,
            seed,
            num_predict,
            top_k,
            top_p,
            tfs_z,
            typical_p,
            repeat_last_n,
            temperature,
            repeat_penalty,
            presence_penalty,
            frequency_penalty,
            mirostat,
            mirostat_tau,
            mirostat_eta,
            penalize_newline,
            stop,
            numa,
            num_ctx,
            num_batch,
            num_gqa,
            num_gpu,
            main_gpu,
            low_vram,
            f16_kv,
            logits_all,
            vocab_only,
            use_mmap,
            use_mlock,
            embedding_only,
            rope_frequency_base,
            rope_frequency_scale,
            num_thread
          ],
          EnumValues: null
        },
        IsRequired: false,
        Summary: Additional model parameters listed in the documentation for the Modelfile such as `temperature`.,
        ParameterName: options,
        ArgumentName: options,
        ParameterDefaultValue: default
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        DefaultValue: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      },
      {
        Id: keep_alive,
        Name: KeepAlive,
        Type: {
          CSharpType: int,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        Summary:
How long (in minutes) to keep the model loaded in memory.

- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.
- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.
- If set to 0, the model will be unloaded immediately once finished.
- If not set, the model will stay loaded for 5 minutes by default
,
        ParameterName: keepAlive,
        ArgumentName: keepAlive,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Generate the next message in a chat with a provided model.,
    RequestType: GenerateChatCompletionRequest,
    ResponseType: GenerateChatCompletionResponse,
    MethodName: GenerateChatCompletionAsync,
    NotAsyncMethodName: GenerateChatCompletion,
    FileNameWithoutExtension: G.Api.EndPoints.GenerateChatCompletion
  },
  {
    Id: GenerateEmbedding,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: false,
    Path: "/embeddings",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: model,
        Name: Model,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ParameterName: model,
        ArgumentName: model,
        ParameterDefaultValue: default
      },
      {
        Id: prompt,
        Name: Prompt,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
Text to generate embeddings for.
<br/>Example: Here is an article about llamas...,
        ParameterName: prompt,
        ArgumentName: prompt,
        ParameterDefaultValue: default
      },
      {
        Id: options,
        Name: Options,
        Type: {
          CSharpType: RequestOptions?,
          IsArray: false,
          IsEnum: false,
          Properties: [
            num_keep,
            seed,
            num_predict,
            top_k,
            top_p,
            tfs_z,
            typical_p,
            repeat_last_n,
            temperature,
            repeat_penalty,
            presence_penalty,
            frequency_penalty,
            mirostat,
            mirostat_tau,
            mirostat_eta,
            penalize_newline,
            stop,
            numa,
            num_ctx,
            num_batch,
            num_gqa,
            num_gpu,
            main_gpu,
            low_vram,
            f16_kv,
            logits_all,
            vocab_only,
            use_mmap,
            use_mlock,
            embedding_only,
            rope_frequency_base,
            rope_frequency_scale,
            num_thread
          ],
          EnumValues: null
        },
        IsRequired: false,
        Summary: Additional model parameters listed in the documentation for the Modelfile such as `temperature`.,
        ParameterName: options,
        ArgumentName: options,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Generate embeddings from a model.,
    RequestType: GenerateEmbeddingRequest,
    ResponseType: GenerateEmbeddingResponse,
    MethodName: GenerateEmbeddingAsync,
    NotAsyncMethodName: GenerateEmbedding,
    FileNameWithoutExtension: G.Api.EndPoints.GenerateEmbedding
  },
  {
    Id: CreateModel,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: true,
    Path: "/create",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: mario,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      },
      {
        Id: modelfile,
        Name: Modelfile,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The contents of the Modelfile.
<br/>Example: FROM llama2\nSYSTEM You are mario from Super Mario Bros.,
        ParameterName: modelfile,
        ArgumentName: modelfile,
        ParameterDefaultValue: default
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        DefaultValue: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Create a model from a Modelfile.,
    RequestType: CreateModelRequest,
    ResponseType: CreateModelResponse,
    MethodName: CreateModelAsync,
    NotAsyncMethodName: CreateModel,
    FileNameWithoutExtension: G.Api.EndPoints.CreateModel
  },
  {
    Id: ListModels,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: false,
    Path: "/tags",
    AuthorizationScheme: ,
    Properties: null,
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: List models that are available locally.,
    RequestType: ,
    ResponseType: ModelsResponse,
    MethodName: ListModelsAsync,
    NotAsyncMethodName: ListModels,
    FileNameWithoutExtension: G.Api.EndPoints.ListModels
  },
  {
    Id: ShowModelInfo,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: false,
    Path: "/show",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Show details about a model including modelfile, template, parameters, license, and system prompt.,
    RequestType: ModelInfoRequest,
    ResponseType: ModelInfo,
    MethodName: ShowModelInfoAsync,
    NotAsyncMethodName: ShowModelInfo,
    FileNameWithoutExtension: G.Api.EndPoints.ShowModelInfo
  },
  {
    Id: CopyModel,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: false,
    Path: "/copy",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: source,
        Name: Source,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
Name of the model to copy.
<br/>Example: llama2:7b,
        ParameterName: source,
        ArgumentName: source,
        ParameterDefaultValue: default
      },
      {
        Id: destination,
        Name: Destination,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
Name of the new model.
<br/>Example: llama2-backup,
        ParameterName: destination,
        ArgumentName: destination,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Creates a model with another name from an existing model.,
    RequestType: CopyModelRequest,
    ResponseType: ,
    MethodName: CopyModelAsync,
    NotAsyncMethodName: CopyModel,
    FileNameWithoutExtension: G.Api.EndPoints.CopyModel
  },
  {
    Id: DeleteModel,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: false,
    Path: "/delete",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:13b,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Delete,
    Summary: Delete a model and its data.,
    RequestType: DeleteModelRequest,
    ResponseType: ,
    MethodName: DeleteModelAsync,
    NotAsyncMethodName: DeleteModel,
    FileNameWithoutExtension: G.Api.EndPoints.DeleteModel
  },
  {
    Id: PullModel,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: false,
    Path: "/pull",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      },
      {
        Id: insecure,
        Name: Insecure,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        DefaultValue: false,
        Summary:
Allow insecure connections to the library. 

Only use this if you are pulling from your own library during development.

<br/>Default Value: false,
        ParameterName: insecure,
        ArgumentName: insecure,
        ParameterDefaultValue: false
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        DefaultValue: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Download a model from the ollama library.,
    RequestType: PullModelRequest,
    ResponseType: PullModelResponse,
    MethodName: PullModelAsync,
    NotAsyncMethodName: PullModel,
    FileNameWithoutExtension: G.Api.EndPoints.PullModel
  },
  {
    Id: PushModel,
    Namespace: G,
    ClassName: Api,
    BaseUrl: ,
    Stream: false,
    Path: "/push",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: true,
        Summary:
The name of the model to push in the form of &lt;namespace&gt;/&lt;model&gt;:&lt;tag&gt;.
<br/>Example: mattw/pygmalion:latest,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      },
      {
        Id: insecure,
        Name: Insecure,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        DefaultValue: false,
        Summary:
Allow insecure connections to the library. 

Only use this if you are pushing to your library during development.

<br/>Default Value: false,
        ParameterName: insecure,
        ArgumentName: insecure,
        ParameterDefaultValue: false
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null
        },
        IsRequired: false,
        DefaultValue: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Upload a model to a model library.,
    RequestType: PushModelRequest,
    ResponseType: PushModelResponse,
    MethodName: PushModelAsync,
    NotAsyncMethodName: PushModel,
    FileNameWithoutExtension: G.Api.EndPoints.PushModel
  },
  {
    Id: Constructors,
    Namespace: G,
    ClassName: Api,
    BaseUrl: http://localhost:11434/api,
    Stream: false,
    Path: ,
    AuthorizationScheme: ,
    Properties: null,
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: ,
    RequestType: ,
    ResponseType: ,
    MethodName: ConstructorsAsync,
    NotAsyncMethodName: Constructors,
    FileNameWithoutExtension: G.Api.EndPoints.Constructors
  }
]