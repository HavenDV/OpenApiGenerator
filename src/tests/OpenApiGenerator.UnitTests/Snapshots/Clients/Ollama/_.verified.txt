[
  {
    Id: GenerateCompletion,
    Namespace: G,
    ClassName: CompletionsClient,
    BaseUrl: ,
    Stream: true,
    Path: "/generate",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: model,
        Name: Model,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ConverterType: ,
        ParameterName: model,
        ArgumentName: model,
        ParameterDefaultValue: default
      },
      {
        Id: prompt,
        Name: Prompt,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The prompt to generate a response.
<br/>Example: Why is the sky blue?,
        ConverterType: ,
        ParameterName: prompt,
        ArgumentName: prompt,
        ParameterDefaultValue: default
      },
      {
        Id: images,
        Name: Images,
        Type: {
          CSharpType: global::System.Collections.Generic.IList<string?>?,
          IsArray: true,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: global::System.Collections.Generic.IList<string?>,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: (optional) a list of Base64-encoded images to include in the message (for multimodal models such as llava),
        ConverterType: ,
        ParameterName: images,
        ArgumentName: images,
        ParameterDefaultValue: default
      },
      {
        Id: system,
        Name: System,
        Type: {
          CSharpType: string?,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: The system prompt to (overrides what is defined in the Modelfile).,
        ConverterType: ,
        ParameterName: system,
        ArgumentName: system,
        ParameterDefaultValue: default
      },
      {
        Id: template,
        Name: Template,
        Type: {
          CSharpType: string?,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: The full prompt or prompt template (overrides what is defined in the Modelfile).,
        ConverterType: ,
        ParameterName: template,
        ArgumentName: template,
        ParameterDefaultValue: default
      },
      {
        Id: context,
        Name: Context,
        Type: {
          CSharpType: global::System.Collections.Generic.IList<int>?,
          IsArray: true,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: global::System.Collections.Generic.IList<int>,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: The context parameter returned from a previous request to [generateCompletion], this can be used to keep a short conversational memory.,
        ConverterType: ,
        ParameterName: context,
        ArgumentName: context,
        ParameterDefaultValue: default
      },
      {
        Id: options,
        Name: Options,
        Type: {
          CSharpType: RequestOptions?,
          IsArray: false,
          IsEnum: false,
          Properties: [
            num_keep,
            seed,
            num_predict,
            top_k,
            top_p,
            tfs_z,
            typical_p,
            repeat_last_n,
            temperature,
            repeat_penalty,
            presence_penalty,
            frequency_penalty,
            mirostat,
            mirostat_tau,
            mirostat_eta,
            penalize_newline,
            stop,
            numa,
            num_ctx,
            num_batch,
            num_gqa,
            num_gpu,
            main_gpu,
            low_vram,
            f16_kv,
            logits_all,
            vocab_only,
            use_mmap,
            use_mlock,
            embedding_only,
            rope_frequency_base,
            rope_frequency_scale,
            num_thread
          ],
          EnumValues: null,
          CSharpTypeWithoutNullability: RequestOptions,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: Additional model parameters listed in the documentation for the Modelfile such as `temperature`.,
        ConverterType: ,
        ParameterName: options,
        ArgumentName: options,
        ParameterDefaultValue: default
      },
      {
        Id: format,
        Name: Format,
        Type: {
          CSharpType: GenerateCompletionRequestFormat?,
          IsArray: false,
          IsEnum: true,
          Properties: [
            Json
          ],
          EnumValues: [
            json
          ],
          CSharpTypeWithoutNullability: GenerateCompletionRequestFormat,
          ConverterType: GenerateCompletionRequestFormatJsonConverter
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary:
The format to return a response in. Currently the only accepted value is json.

Enable JSON mode by setting the format parameter to json. This will structure the response as valid JSON.

Note: it's important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.
,
        ConverterType: GenerateCompletionRequestFormatJsonConverter,
        ParameterName: format,
        ArgumentName: format,
        ParameterDefaultValue: default
      },
      {
        Id: raw,
        Name: Raw,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary:
If `true` no formatting will be applied to the prompt and no context will be returned. 

You may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API, and are managing history yourself.
,
        ConverterType: ,
        ParameterName: raw,
        ArgumentName: raw,
        ParameterDefaultValue: default
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        DefaultValue: false,
        IsDeprecated: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ConverterType: ,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      },
      {
        Id: keep_alive,
        Name: KeepAlive,
        Type: {
          CSharpType: int,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: int,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary:
How long (in minutes) to keep the model loaded in memory.

- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.
- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.
- If set to 0, the model will be unloaded immediately once finished.
- If not set, the model will stay loaded for 5 minutes by default
,
        ConverterType: ,
        ParameterName: keepAlive,
        ArgumentName: keepAlive,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Generate a response for a given prompt with a provided model.,
    RequestType: {
      CSharpType: GenerateCompletionRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        model,
        prompt,
        images,
        system,
        template,
        context,
        options,
        format,
        raw,
        stream,
        keep_alive
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: GenerateCompletionRequest,
      ConverterType: 
    },
    ResponseType: GenerateCompletionResponse,
    AdditionalModels: null,
    MethodName: GenerateCompletionAsync,
    NotAsyncMethodName: GenerateCompletion,
    FileNameWithoutExtension: G.CompletionsClient.GenerateCompletion
  },
  {
    Id: GenerateChatCompletion,
    Namespace: G,
    ClassName: ChatClient,
    BaseUrl: ,
    Stream: true,
    Path: "/chat",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: model,
        Name: Model,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ConverterType: ,
        ParameterName: model,
        ArgumentName: model,
        ParameterDefaultValue: default
      },
      {
        Id: messages,
        Name: Messages,
        Type: {
          CSharpType: global::System.Collections.Generic.IList<Message>,
          IsArray: true,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: global::System.Collections.Generic.IList<Message>,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary: The messages of the chat, this can be used to keep a chat memory,
        ConverterType: ,
        ParameterName: messages,
        ArgumentName: messages,
        ParameterDefaultValue: default
      },
      {
        Id: format,
        Name: Format,
        Type: {
          CSharpType: GenerateChatCompletionRequestFormat?,
          IsArray: false,
          IsEnum: true,
          Properties: [
            Json
          ],
          EnumValues: [
            json
          ],
          CSharpTypeWithoutNullability: GenerateChatCompletionRequestFormat,
          ConverterType: GenerateChatCompletionRequestFormatJsonConverter
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary:
The format to return a response in. Currently the only accepted value is json.

Enable JSON mode by setting the format parameter to json. This will structure the response as valid JSON.

Note: it's important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.
,
        ConverterType: GenerateChatCompletionRequestFormatJsonConverter,
        ParameterName: format,
        ArgumentName: format,
        ParameterDefaultValue: default
      },
      {
        Id: options,
        Name: Options,
        Type: {
          CSharpType: RequestOptions?,
          IsArray: false,
          IsEnum: false,
          Properties: [
            num_keep,
            seed,
            num_predict,
            top_k,
            top_p,
            tfs_z,
            typical_p,
            repeat_last_n,
            temperature,
            repeat_penalty,
            presence_penalty,
            frequency_penalty,
            mirostat,
            mirostat_tau,
            mirostat_eta,
            penalize_newline,
            stop,
            numa,
            num_ctx,
            num_batch,
            num_gqa,
            num_gpu,
            main_gpu,
            low_vram,
            f16_kv,
            logits_all,
            vocab_only,
            use_mmap,
            use_mlock,
            embedding_only,
            rope_frequency_base,
            rope_frequency_scale,
            num_thread
          ],
          EnumValues: null,
          CSharpTypeWithoutNullability: RequestOptions,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: Additional model parameters listed in the documentation for the Modelfile such as `temperature`.,
        ConverterType: ,
        ParameterName: options,
        ArgumentName: options,
        ParameterDefaultValue: default
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        DefaultValue: false,
        IsDeprecated: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ConverterType: ,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      },
      {
        Id: keep_alive,
        Name: KeepAlive,
        Type: {
          CSharpType: int,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: int,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary:
How long (in minutes) to keep the model loaded in memory.

- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.
- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.
- If set to 0, the model will be unloaded immediately once finished.
- If not set, the model will stay loaded for 5 minutes by default
,
        ConverterType: ,
        ParameterName: keepAlive,
        ArgumentName: keepAlive,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Generate the next message in a chat with a provided model.,
    RequestType: {
      CSharpType: GenerateChatCompletionRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        model,
        messages,
        format,
        options,
        stream,
        keep_alive
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: GenerateChatCompletionRequest,
      ConverterType: 
    },
    ResponseType: GenerateChatCompletionResponse,
    AdditionalModels: null,
    MethodName: GenerateChatCompletionAsync,
    NotAsyncMethodName: GenerateChatCompletion,
    FileNameWithoutExtension: G.ChatClient.GenerateChatCompletion
  },
  {
    Id: GenerateEmbedding,
    Namespace: G,
    ClassName: EmbeddingsClient,
    BaseUrl: ,
    Stream: false,
    Path: "/embeddings",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: model,
        Name: Model,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ConverterType: ,
        ParameterName: model,
        ArgumentName: model,
        ParameterDefaultValue: default
      },
      {
        Id: prompt,
        Name: Prompt,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
Text to generate embeddings for.
<br/>Example: Here is an article about llamas...,
        ConverterType: ,
        ParameterName: prompt,
        ArgumentName: prompt,
        ParameterDefaultValue: default
      },
      {
        Id: options,
        Name: Options,
        Type: {
          CSharpType: RequestOptions?,
          IsArray: false,
          IsEnum: false,
          Properties: [
            num_keep,
            seed,
            num_predict,
            top_k,
            top_p,
            tfs_z,
            typical_p,
            repeat_last_n,
            temperature,
            repeat_penalty,
            presence_penalty,
            frequency_penalty,
            mirostat,
            mirostat_tau,
            mirostat_eta,
            penalize_newline,
            stop,
            numa,
            num_ctx,
            num_batch,
            num_gqa,
            num_gpu,
            main_gpu,
            low_vram,
            f16_kv,
            logits_all,
            vocab_only,
            use_mmap,
            use_mlock,
            embedding_only,
            rope_frequency_base,
            rope_frequency_scale,
            num_thread
          ],
          EnumValues: null,
          CSharpTypeWithoutNullability: RequestOptions,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: Additional model parameters listed in the documentation for the Modelfile such as `temperature`.,
        ConverterType: ,
        ParameterName: options,
        ArgumentName: options,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Generate embeddings from a model.,
    RequestType: {
      CSharpType: GenerateEmbeddingRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        model,
        prompt,
        options
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: GenerateEmbeddingRequest,
      ConverterType: 
    },
    ResponseType: GenerateEmbeddingResponse,
    AdditionalModels: null,
    MethodName: GenerateEmbeddingAsync,
    NotAsyncMethodName: GenerateEmbedding,
    FileNameWithoutExtension: G.EmbeddingsClient.GenerateEmbedding
  },
  {
    Id: CreateModel,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: ,
    Stream: true,
    Path: "/create",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: mario,
        ConverterType: ,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      },
      {
        Id: modelfile,
        Name: Modelfile,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The contents of the Modelfile.
<br/>Example: FROM llama2\nSYSTEM You are mario from Super Mario Bros.,
        ConverterType: ,
        ParameterName: modelfile,
        ArgumentName: modelfile,
        ParameterDefaultValue: default
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        DefaultValue: false,
        IsDeprecated: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ConverterType: ,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Create a model from a Modelfile.,
    RequestType: {
      CSharpType: CreateModelRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        name,
        modelfile,
        stream
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: CreateModelRequest,
      ConverterType: 
    },
    ResponseType: CreateModelResponse,
    AdditionalModels: null,
    MethodName: CreateModelAsync,
    NotAsyncMethodName: CreateModel,
    FileNameWithoutExtension: G.ModelsClient.CreateModel
  },
  {
    Id: ListModels,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: ,
    Stream: false,
    Path: "/tags",
    AuthorizationScheme: ,
    Properties: null,
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: List models that are available locally.,
    RequestType: {
      CSharpType: ,
      IsArray: false,
      IsEnum: false,
      Properties: null,
      EnumValues: null,
      CSharpTypeWithoutNullability: ,
      ConverterType: 
    },
    ResponseType: ModelsResponse,
    AdditionalModels: null,
    MethodName: ListModelsAsync,
    NotAsyncMethodName: ListModels,
    FileNameWithoutExtension: G.ModelsClient.ListModels
  },
  {
    Id: ShowModelInfo,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: ,
    Stream: false,
    Path: "/show",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ConverterType: ,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Show details about a model including modelfile, template, parameters, license, and system prompt.,
    RequestType: {
      CSharpType: ModelInfoRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        name
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: ModelInfoRequest,
      ConverterType: 
    },
    ResponseType: ModelInfo,
    AdditionalModels: null,
    MethodName: ShowModelInfoAsync,
    NotAsyncMethodName: ShowModelInfo,
    FileNameWithoutExtension: G.ModelsClient.ShowModelInfo
  },
  {
    Id: CopyModel,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: ,
    Stream: false,
    Path: "/copy",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: source,
        Name: Source,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
Name of the model to copy.
<br/>Example: llama2:7b,
        ConverterType: ,
        ParameterName: source,
        ArgumentName: source,
        ParameterDefaultValue: default
      },
      {
        Id: destination,
        Name: Destination,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
Name of the new model.
<br/>Example: llama2-backup,
        ConverterType: ,
        ParameterName: destination,
        ArgumentName: destination,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Creates a model with another name from an existing model.,
    RequestType: {
      CSharpType: CopyModelRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        source,
        destination
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: CopyModelRequest,
      ConverterType: 
    },
    ResponseType: ,
    AdditionalModels: null,
    MethodName: CopyModelAsync,
    NotAsyncMethodName: CopyModel,
    FileNameWithoutExtension: G.ModelsClient.CopyModel
  },
  {
    Id: DeleteModel,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: ,
    Stream: false,
    Path: "/delete",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:13b,
        ConverterType: ,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Delete,
    Summary: Delete a model and its data.,
    RequestType: {
      CSharpType: DeleteModelRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        name
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: DeleteModelRequest,
      ConverterType: 
    },
    ResponseType: ,
    AdditionalModels: null,
    MethodName: DeleteModelAsync,
    NotAsyncMethodName: DeleteModel,
    FileNameWithoutExtension: G.ModelsClient.DeleteModel
  },
  {
    Id: PullModel,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: ,
    Stream: false,
    Path: "/pull",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The model name. 

Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

<br/>Example: llama2:7b,
        ConverterType: ,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      },
      {
        Id: insecure,
        Name: Insecure,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        DefaultValue: false,
        IsDeprecated: false,
        Summary:
Allow insecure connections to the library. 

Only use this if you are pulling from your own library during development.

<br/>Default Value: false,
        ConverterType: ,
        ParameterName: insecure,
        ArgumentName: insecure,
        ParameterDefaultValue: false
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        DefaultValue: false,
        IsDeprecated: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ConverterType: ,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Download a model from the ollama library.,
    RequestType: {
      CSharpType: PullModelRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        name,
        insecure,
        stream
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: PullModelRequest,
      ConverterType: 
    },
    ResponseType: PullModelResponse,
    AdditionalModels: null,
    MethodName: PullModelAsync,
    NotAsyncMethodName: PullModel,
    FileNameWithoutExtension: G.ModelsClient.PullModel
  },
  {
    Id: PushModel,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: ,
    Stream: false,
    Path: "/push",
    AuthorizationScheme: ,
    Properties: [
      {
        Id: name,
        Name: Name,
        Type: {
          CSharpType: string,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: string,
          ConverterType: 
        },
        IsRequired: true,
        IsDeprecated: false,
        Summary:
The name of the model to push in the form of &lt;namespace&gt;/&lt;model&gt;:&lt;tag&gt;.
<br/>Example: mattw/pygmalion:latest,
        ConverterType: ,
        ParameterName: name,
        ArgumentName: name,
        ParameterDefaultValue: default
      },
      {
        Id: insecure,
        Name: Insecure,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        DefaultValue: false,
        IsDeprecated: false,
        Summary:
Allow insecure connections to the library. 

Only use this if you are pushing to your library during development.

<br/>Default Value: false,
        ConverterType: ,
        ParameterName: insecure,
        ArgumentName: insecure,
        ParameterDefaultValue: false
      },
      {
        Id: stream,
        Name: Stream,
        Type: {
          CSharpType: bool,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: bool,
          ConverterType: 
        },
        IsRequired: false,
        DefaultValue: false,
        IsDeprecated: false,
        Summary:
If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.

<br/>Default Value: false,
        ConverterType: ,
        ParameterName: stream,
        ArgumentName: stream,
        ParameterDefaultValue: false
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    HttpMethod: Post,
    Summary: Upload a model to a model library.,
    RequestType: {
      CSharpType: PushModelRequest,
      IsArray: false,
      IsEnum: false,
      Properties: [
        name,
        insecure,
        stream
      ],
      EnumValues: null,
      CSharpTypeWithoutNullability: PushModelRequest,
      ConverterType: 
    },
    ResponseType: PushModelResponse,
    AdditionalModels: null,
    MethodName: PushModelAsync,
    NotAsyncMethodName: PushModel,
    FileNameWithoutExtension: G.ModelsClient.PushModel
  },
  {
    Id: Constructors,
    Namespace: G,
    ClassName: Api,
    BaseUrl: http://localhost:11434/api,
    Stream: false,
    Path: ,
    AuthorizationScheme: ,
    Properties: [
      {
        Id: ,
        Name: Completions,
        Type: {
          CSharpType: CompletionsClient,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: CompletionsClient,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: Given a prompt, the model will generate a completion.,
        ConverterType: ,
        ParameterName: completions,
        ArgumentName: completions,
        ParameterDefaultValue: default
      },
      {
        Id: ,
        Name: Chat,
        Type: {
          CSharpType: ChatClient,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: ChatClient,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: Given a list of messages comprising a conversation, the model will return a response.,
        ConverterType: ,
        ParameterName: chat,
        ArgumentName: chat,
        ParameterDefaultValue: default
      },
      {
        Id: ,
        Name: Embeddings,
        Type: {
          CSharpType: EmbeddingsClient,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: EmbeddingsClient,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: Get a vector representation of a given input.,
        ConverterType: ,
        ParameterName: embeddings,
        ArgumentName: embeddings,
        ParameterDefaultValue: default
      },
      {
        Id: ,
        Name: Models,
        Type: {
          CSharpType: ModelsClient,
          IsArray: false,
          IsEnum: false,
          Properties: null,
          EnumValues: null,
          CSharpTypeWithoutNullability: ModelsClient,
          ConverterType: 
        },
        IsRequired: false,
        IsDeprecated: false,
        Summary: List and describe the various models available.,
        ConverterType: ,
        ParameterName: models,
        ArgumentName: models,
        ParameterDefaultValue: default
      }
    ],
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: API Spec for Ollama API. Please see https://github.com/jmorganca/ollama/blob/main/docs/api.md for more details.,
    RequestType: {
      CSharpType: ,
      IsArray: false,
      IsEnum: false,
      Properties: null,
      EnumValues: null,
      CSharpTypeWithoutNullability: ,
      ConverterType: 
    },
    ResponseType: ,
    AdditionalModels: null,
    MethodName: ConstructorsAsync,
    NotAsyncMethodName: Constructors,
    FileNameWithoutExtension: G.Api
  },
  {
    Id: Constructors,
    Namespace: G,
    ClassName: CompletionsClient,
    BaseUrl: http://localhost:11434/api,
    Stream: false,
    Path: ,
    AuthorizationScheme: ,
    Properties: null,
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: Given a prompt, the model will generate a completion.,
    RequestType: {
      CSharpType: ,
      IsArray: false,
      IsEnum: false,
      Properties: null,
      EnumValues: null,
      CSharpTypeWithoutNullability: ,
      ConverterType: 
    },
    ResponseType: ,
    AdditionalModels: null,
    MethodName: ConstructorsAsync,
    NotAsyncMethodName: Constructors,
    FileNameWithoutExtension: G.CompletionsClient
  },
  {
    Id: Constructors,
    Namespace: G,
    ClassName: ChatClient,
    BaseUrl: http://localhost:11434/api,
    Stream: false,
    Path: ,
    AuthorizationScheme: ,
    Properties: null,
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: Given a list of messages comprising a conversation, the model will return a response.,
    RequestType: {
      CSharpType: ,
      IsArray: false,
      IsEnum: false,
      Properties: null,
      EnumValues: null,
      CSharpTypeWithoutNullability: ,
      ConverterType: 
    },
    ResponseType: ,
    AdditionalModels: null,
    MethodName: ConstructorsAsync,
    NotAsyncMethodName: Constructors,
    FileNameWithoutExtension: G.ChatClient
  },
  {
    Id: Constructors,
    Namespace: G,
    ClassName: EmbeddingsClient,
    BaseUrl: http://localhost:11434/api,
    Stream: false,
    Path: ,
    AuthorizationScheme: ,
    Properties: null,
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: Get a vector representation of a given input.,
    RequestType: {
      CSharpType: ,
      IsArray: false,
      IsEnum: false,
      Properties: null,
      EnumValues: null,
      CSharpTypeWithoutNullability: ,
      ConverterType: 
    },
    ResponseType: ,
    AdditionalModels: null,
    MethodName: ConstructorsAsync,
    NotAsyncMethodName: Constructors,
    FileNameWithoutExtension: G.EmbeddingsClient
  },
  {
    Id: Constructors,
    Namespace: G,
    ClassName: ModelsClient,
    BaseUrl: http://localhost:11434/api,
    Stream: false,
    Path: ,
    AuthorizationScheme: ,
    Properties: null,
    TargetFramework: netstandard2.0,
    JsonSerializerContext: ,
    Summary: List and describe the various models available.,
    RequestType: {
      CSharpType: ,
      IsArray: false,
      IsEnum: false,
      Properties: null,
      EnumValues: null,
      CSharpTypeWithoutNullability: ,
      ConverterType: 
    },
    ResponseType: ,
    AdditionalModels: null,
    MethodName: ConstructorsAsync,
    NotAsyncMethodName: Constructors,
    FileNameWithoutExtension: G.ModelsClient
  }
]